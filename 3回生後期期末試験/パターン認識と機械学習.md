
k-means法
データをK個のグループに分類する教師なし学習である。
各データを最も近い重心に割り当てる「ハードクラスタリング」を行い、重心と距離の二乗和を最小化する。計算は高速だが、クラスタの形状として球状を仮定するため複雑な形状の分類には向かない。

GMM
データが複数の正規分布の重ね合わせから生成されたと仮定する確率モデルである。EMアルゴリズムを用いてパラメータを推定し、各データを確率的に割り当てる「ソフトクラスタリング」を行う。分散共分散行列を使うため、楕円形や斜めの分布も柔軟に表現できる。

理論的にはk-meansはGMMの特殊なケースに相当する。GMMにおいて全ての分布の形状を球状に固定し、分散を0に近づける極限を取ると、数学的にk-meansと等価になる。実用的には、単純な円形データを高速に分けたいならk-means、分布の重なりや歪みを考慮したいならGMMが適している。


RNN
時系列データを先頭から順に1つずつ処理し、過去の情報を「隠れ状態」として保持しながら次へ受け渡すモデル。処理が逐次的であるため並列計算ができず、系列長が長くなると過去の情報が薄れてしまう問題がある。

Transformer
再帰的構造を完全に廃止し、Self-Attentionのみを用いたモデル。入力された全てのトークンの関係性を一括で計算できるため遠く離れたトークン同士の文脈も直接かつ正確に捉えることができ、GPUによる並列処理で学習速度も飛躍的に向上した。

RNNが前から順に処理する（逐次処理）のに対し、Transformerは系列全体を同時に処理する（並列処理）。RNNは計算が遅かったり長期記憶に限界があったが、Transformerはそれらの問題を克服した。


AE（オートエンコーダ）
入力データを圧縮（エンコード）して特徴を抽出し、再び元に戻す（デコード）ニューラルネットワークである。次元圧縮やノイズ除去、異常検知などに利用される。しかし圧縮されたデータの分布が歪になりやすく、未知のデータを生成することには適していない。

VAE（変分オートエンコーダ）
AEを確率モデルへと拡張した手法。データを一転の座標ではなく確率分布（平均と分散）として潜在空間にマッピングする。これにより潜在空間が隙間なく滑らかに構成されるため、そこからランダムにサンプリングすることで、学習データに似た新しいデータを生成することができる。

VAEはAEの発展形だが、アプローチが異なる。AEがデータを確定的な点として扱うのに対し、VAEは確率的な広がりとして扱う。VAEは、圧縮だけでなく、画像生成やデータ間の滑らかな補完といったタスクを実現できる。